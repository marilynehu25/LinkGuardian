import asyncio
from datetime import datetime

from aiohttp import ClientError, ClientSession

# Importer Celery depuis le fichier d√©di√©
from celery_app import celery
from models import User, Website,TaskRecord
from services.api_babbar import fetch_url_data
from database import db

# üîß CONFIGURATION DES LIMITES D'API
API_RATE_LIMITS = {
    "babbar": {"calls_per_minute": 10, "retry_after": 60},
    "google": {"calls_per_minute": 20, "retry_after": 30},
    "default": {"calls_per_minute": 10, "retry_after": 60},
}


class APIRateLimitError(Exception):
    """Exception lev√©e quand on atteint une limite d'API"""

    def __init__(self, api_name, retry_after=60):
        self.api_name = api_name
        self.retry_after = retry_after
        super().__init__(
            f"Rate limit atteint pour {api_name}. Retry apr√®s {retry_after}s"
        )


async def process_site_async(site_id):
    from services.api_serpapi import check_google_indexation
    from services.check_service import check_link_presence_and_follow_status_async

    site = Website.query.get(site_id)
    if not site:
        return {"success": False, "site_id": site_id, "error": "Site non trouv√©"}

    async with ClientSession() as session:
        try:
            # 1) V√©rifications HTML
            link_data = await check_link_presence_and_follow_status_async(
                session, site.url, site.link_to_check, site.anchor_text
            )

            index_status = await check_google_indexation(session, site.url)

            link_present, anchor_present, follow_status, status_code = link_data

            # Enregistrer l'ancien √©tat
            old_site = Website(
                url=site.url,
                link_to_check=site.link_to_check,
                anchor_text=site.anchor_text,
                link_status=site.link_status,
                link_follow_status=site.link_follow_status,
                anchor_status=site.anchor_status,
                google_index_status=site.google_index_status,
                source_plateforme=site.source_plateforme,
                last_checked=site.last_checked,
                user_id=site.user_id,
                page_value=site.page_value,
                page_trust=site.page_trust,
                bas=site.bas,
                backlinks_external=site.backlinks_external,
                num_outlinks_ext=site.num_outlinks_ext,
                status_code=site.status_code,
                tag=site.tag,
            )

            # 2) Mise √† jour partielle
            site.status_code = status_code
            site.link_status = "Lien pr√©sent" if link_present else "URL non pr√©sente"
            site.link_follow_status = follow_status if link_present else None
            site.anchor_status = (
                "Ancre pr√©sente" if anchor_present else "Ancre manquante"
            )
            site.google_index_status = index_status

            # 3) R√©cup√©ration Babbar AVANT commit
            try:
                fetch_url_data(site.url, async_mode=False)
            except Exception as e:
                err = str(e).lower()
                if "limit" in err or "429" in err:
                    raise APIRateLimitError("babbar", retry_after=60)
                else:
                    print(f"‚ö†Ô∏è Erreur Babbar non critique : {e}")

            # 4) last_checked ‚Äî maintenant OK
            site.last_checked = datetime.now()
            if not site.first_checked:
                site.first_checked = datetime.now()

            # 5) Commit FINAL UNIQUE
            db.session.commit()

            # Ajouter l'historique
            db.session.add(old_site)
            db.session.commit()

            return {"success": True, "site_id": site.id}

        except APIRateLimitError:
            db.session.rollback()
            raise

        except Exception as e:
            db.session.rollback()
            print(f"‚ùå Erreur pour {site.url}: {e}")
            raise


@celery.task(
    name="tasks.check_single_site",
    bind=True,
    max_retries=5,
    default_retry_delay=60,
    rate_limit="15/m",
    autoretry_for=(APIRateLimitError, ClientError),
    retry_backoff=True,
    retry_backoff_max=600,
    retry_jitter=True,
)
def check_single_site(self, site_id):
    try:
        print(f"üîç V√©rification site ID: {site_id}")

        site = Website.query.get(site_id)
        if not site:
            print(f"‚è≠Ô∏è Site {site_id} supprim√© ‚Äî t√¢che termin√©e")
            return {"success": True, "skipped": True}

        result = asyncio.run(process_site_async(site_id))
        print(f"‚úÖ V√©rification OK pour {site_id}")
        return result

    except APIRateLimitError as exc:
        print(f"‚è≥ Rate limit pour site {site_id}, retry dans {exc.retry_after}s")
        raise self.retry(exc=exc, countdown=exc.retry_after)

    except Exception as exc:
        site = Website.query.get(site_id)
        if site and self.request.retries < self.max_retries:
            print(f"üîÑ Erreur {exc}, retry‚Ä¶")
            raise self.retry(exc=exc)
        print(f"‚ùå Abandon du site {site_id}")
        return {"success": False, "error": str(exc)}


@celery.task(
    name="tasks.check_all_user_sites",
    rate_limit="10/m",  # ‚¨ÜÔ∏è Augment√© de 2/m √† 10/m
)
def check_all_user_sites(user_id, urgent=False):
    """V√©rifie tous les sites d'un utilisateur

    üöÄ OPTIMISATION: Les t√¢ches sont lanc√©es sans countdown.
    Les workers multiples se r√©partissent automatiquement la charge.

    Args:
        user_id: ID de l'utilisateur
        urgent: Si True, les v√©rifications seront prioritaires
    """
    print(f"üìÑ D√©but v√©rification pour l'utilisateur {user_id}")

    sites = Website.query.filter_by(user_id=user_id).all()
    total_sites = len(sites)
    print(f"üìä {total_sites} sites √† v√©rifier")

    if total_sites == 0:
        return {
            "user_id": user_id,
            "total_sites": 0,
            "planned_tasks": 0,
            "skipped_sites": 0,
            "task_ids": [],
        }

    task_ids = []
    skipped = 0

    # üöÄ STRAT√âGIE: Lancer toutes les t√¢ches imm√©diatement
    # Les workers multiples vont se r√©partir le travail automatiquement
    for i, site in enumerate(sites):
        # üßπ V√©rifie que le site est encore valide
        if not site or not site.url:
            skipped += 1
            continue

        # ‚úÖ Lancer la t√¢che SANS countdown
        # Le syst√®me de queues et les multiples workers g√©reront la distribution
        task = check_single_site.apply_async(
            args=[site.id],
            kwargs={"urgent": urgent},
            queue="urgent" if urgent else "standard",  # Routing vers bonne queue
            priority=9 if urgent else 5,  # Priorit√© explicite
        )
        task_ids.append(task.id)

        db.session.add(TaskRecord(task_id=task.id, user_id=user_id))

        # Log tous les 25 sites
        if (i + 1) % 25 == 0:
            print(f"  ‚è≥ {i + 1}/{total_sites} t√¢ches planifi√©es...")

    db.session.commit()

    print(f"‚úÖ {len(task_ids)} t√¢ches lanc√©es ({skipped} sites ignor√©s).")
    print(f"üî• Mode: {'URGENT (priorit√© haute)' if urgent else 'STANDARD'}")

    # Snapshot des stats
    from services.stats_service import save_stats_snapshot

    save_stats_snapshot(user_id)

    return {
        "user_id": user_id,
        "total_sites": total_sites,
        "planned_tasks": len(task_ids),
        "skipped_sites": skipped,
        "task_ids": task_ids,
        "mode": "urgent" if urgent else "standard",
    }


@celery.task(name="tasks.check_all_sites_weekly")
def check_all_sites_weekly():
    """V√©rification hebdomadaire automatique

    üéØ OPTIMISATION: Espacement entre utilisateurs r√©duit de 30min √† 5min
    Les workers multiples peuvent g√©rer plusieurs utilisateurs simultan√©ment
    """
    print("‚è∞ D√©but v√©rification hebdomadaire")

    users = User.query.all()
    total_users = len(users)

    print(f"üë• {total_users} utilisateurs trouv√©s")

    # üöÄ Espacement r√©duit : 5 minutes entre chaque utilisateur
    # Avec 3+ workers, plusieurs utilisateurs seront trait√©s en parall√®le
    for i, user in enumerate(users):
        countdown = i * 300  # 300s = 5 minutes (au lieu de 30)

        print(
            f"üìÖ V√©rification user {user.id} planifi√©e dans {countdown / 60:.0f} minutes"
        )

        check_all_user_sites.apply_async(
            args=[user.id],
            countdown=countdown,
            queue="weekly",  # Queue d√©di√©e basse priorit√©
        )

    total_duration_hours = (total_users * 5) / 60
    print(f"‚úÖ V√©rifications lanc√©es pour {total_users} utilisateurs")
    print(f"‚è±Ô∏è Dur√©e estim√©e totale: {total_duration_hours:.1f} heures")

    return {
        "total_users": total_users,
        "message": "V√©rification hebdomadaire lanc√©e",
        "estimated_duration_hours": total_duration_hours,
    }


@celery.task(name="tasks.check_task_status")
def check_task_status(task_id):
    """V√©rifie le statut d'une t√¢che"""
    from celery.result import AsyncResult

    result = AsyncResult(task_id, app=celery)

    return {
        "task_id": task_id,
        "state": result.state,
        "result": result.result if result.ready() else None,
        "traceback": result.traceback if result.failed() else None,
    }
